{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DifferentLee/WeedDensityModelMediaPipeBackup/blob/main/WeedDensityModelMediaPipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9uA_GxMEdo"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "The MediaPipe image classification solution provides several models you can use immediately for machine learning (ML) in your application. However, if you need to classify images with content not covered by the provided models, you can customize any of the provided models with your own data and MediaPipe [Model Maker](https://developers.google.com/mediapipe/solutions/model_maker). This model modification tool rebuilds a portion of the model using data you provide. This method is faster than training a new model and can produce a model that is more useful for your specific application.\n",
        "\n",
        "The following sections show you how to use Model Maker to retrain a pre-built model for image classification with your own data, which you can then use with the MediaPipe [Image Classifier](https://developers.google.com/mediapipe/solutions/vision/image_classifier). The example retrains a general purpose classification model to classify images of flowers.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing an ImageNet pretrained image classification model for recognizing flowers defined in a user customized flower dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZRdscPmRAq8"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section describes key steps for setting up your development environment to retrain a model. These instructions describe how to update a model using [Google Colab](https://colab.research.google.com/), and you can also use Python in your own development environment. For general information on setting up your development environment for using MediaPipe, including platform version requirements, see the [Setup guide for Python](https://developers.google.com/mediapipe/solutions/setup_python).\n",
        "\n",
        "To install the libraries for customizing a model, run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# 压缩文件路径\n",
        "zip_file_path = '/content/image_picture.zip'\n",
        "\n",
        "# 解压缩目标文件夹路径\n",
        "extract_to_folder = '/content/image_picture/'\n",
        "\n",
        "# 打开压缩文件\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # 解压缩文件到指定目录\n",
        "    zip_ref.extractall(extract_to_folder)\n",
        "\n",
        "print(\"解压缩完成。\")"
      ],
      "metadata": {
        "id": "Ji8gl4GsCgU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa657c65-1943-42be-f163-e5463a53246e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解压缩完成。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plvO-YmcQn5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "809e3c4a-5d99-44e9-f681-f04b7529c5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting keras<3.0.0\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: mediapipe-model-maker in /usr/local/lib/python3.10/dist-packages (0.2.1.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n",
            "Requirement already satisfied: mediapipe>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.10.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.8.0.76)\n",
            "Requirement already satisfied: tensorflow>=2.10 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.23.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.4)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.16.1)\n",
            "Requirement already satisfied: tf-models-official>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.16.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (2.2.1+cu121)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (24.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.16.2)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow>=2.10 (from mediapipe-model-maker)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.36.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow>=2.10->mediapipe-model-maker)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.15.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (3.0.10)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.84.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.2.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.9.0.80)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (6.0.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.4.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.99)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.16.1)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.16.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.1.0)\n",
            "Collecting tf-models-official>=2.13.1 (from mediapipe-model-maker)\n",
            "  Using cached tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting tensorflow-text~=2.15.0 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->mediapipe-model-maker) (2.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.8)\n",
            "Requirement already satisfied: etils>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (1.7.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (4.66.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10->mediapipe-model-maker) (0.43.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (3.18.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (6.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official>=2.13.1->mediapipe-model-maker) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.10->mediapipe-model-maker) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.10->mediapipe-model-maker) (3.6)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub->mediapipe-model-maker)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.1.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->mediapipe>=0.10.0->mediapipe-model-maker) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mediapipe>=0.10.0->mediapipe-model-maker) (12.4.127)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (5.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (1.3.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (2.1.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mediapipe>=0.10.0->mediapipe-model-maker) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.10->mediapipe-model-maker) (3.2.2)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras, tensorboard, tensorflow, tf-keras, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.2.1\n",
            "    Uninstalling keras-3.2.1:\n",
            "      Successfully uninstalled keras-3.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.16.2\n",
            "    Uninstalling tensorboard-2.16.2:\n",
            "      Successfully uninstalled tensorboard-2.16.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.16.1\n",
            "    Uninstalling tensorflow-2.16.1:\n",
            "      Successfully uninstalled tensorflow-2.16.1\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.16.0\n",
            "    Uninstalling tf_keras-2.16.0:\n",
            "      Successfully uninstalled tf_keras-2.16.0\n",
            "  Attempting uninstall: tensorflow-text\n",
            "    Found existing installation: tensorflow-text 2.16.1\n",
            "    Uninstalling tensorflow-text-2.16.1:\n",
            "      Successfully uninstalled tensorflow-text-2.16.1\n",
            "  Attempting uninstall: tf-models-official\n",
            "    Found existing installation: tf-models-official 2.16.0\n",
            "    Uninstalling tf-models-official-2.16.0:\n",
            "      Successfully uninstalled tf-models-official-2.16.0\n",
            "Successfully installed keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-text-2.15.0 tf-keras-2.15.1 tf-models-official-2.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "official",
                  "tensorflow",
                  "tf_keras"
                ]
              },
              "id": "68cb30029d044d6d9a1e0f7159ae868c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "# !pip install mediapipe-model-maker\n",
        "!pip install 'keras<3.0.0' mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras"
      ],
      "metadata": {
        "id": "hyDOdbs3IIZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1baef59-8d3c-478d-912b-119a53ba3761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RikfKdBiCDqF",
        "outputId": "dd3e14e6-77dd-456c-dd70-6ab286758059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbu3mnPiSvSn"
      },
      "source": [
        "Use the following code to import the required Python classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG2McL-NEOpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41653669-9e36-47f3-8483-47a52149a31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras.engine import keras_tensor\n",
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import image_classifier\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7rcwtC5TQQ1"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Retraining a model for image classification requires a dataset that includes all kinds of items, or *classes*, that you want the completed model to be able to identify. You can do this by trimming down a public dataset to only the classes that are relevant to your usecase, compiling your own data, or some combination of both. The dataset can be significantly smaller that what would be required to train a new model. For example, the [ImageNet](https://www.image-net.org) dataset used to train many reference models contains millions of images with thousands of categories. Transfer learning with Model Maker can retrain an existing model with a smaller dataset and still perform well, depending on your inference accuracy goals. These instructions use a smaller dataset containing 5 types of flowers, or 5 _classes_, with 600 to 800 images per class.\n",
        "\n",
        "To download the example dataset, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Eu0PHlOWcvc"
      },
      "outputs": [],
      "source": [
        "# image_path = tf.keras.utils.get_file(\n",
        "#     'flower_photos.tgz',\n",
        "#     'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "#     extract=True)\n",
        "# image_path = os.path.join(os.path.dirname(image_path), 'flower_photos')\n",
        "image_path = '/content/image_picture/image_picture'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KLJjaWQTosl"
      },
      "source": [
        "This code stores the downloaded images at the directory location saved in the `image_path` variable. That directory contains several subdirectories, each corresponding to specific class labels. Your training data should also follow this pattern: `<image_path>/<label_name>/<image_names>.*`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QghwXAtTCGO"
      },
      "source": [
        "### Review data\n",
        "\n",
        "When preparing data for training with Model Maker, you should review the training data to make sure it is in the proper format, correctly classified, and organized in directories corresponding to classification labels. *This step is optional, but recommended.*\n",
        "\n",
        "The following code block retrieves all the label names from the expected directory structure at `image_path` and prints them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65jYI0XtyiJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcec93e1-74e0-480e-da7b-7fafed6bf8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image_picture/image_picture\n",
            "['bahiagrass_only', 'all_weeds_in_bahiagrass']\n"
          ]
        }
      ],
      "source": [
        "print(image_path)\n",
        "labels = []\n",
        "for i in os.listdir(image_path):\n",
        "  if os.path.isdir(os.path.join(image_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdVEhza1KkbJ"
      },
      "source": [
        "You can review a few of the example images from each category using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqj6ydhnGsgy"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 5\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(image_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMAShPQWUsbc"
      },
      "source": [
        "### Create dataset\n",
        "\n",
        "Training data for machine learning can be large, consisting of hundreds or thousands of files which typically do not fit into available memory. You must also split it into groups for different uses: training, testing, and validation. For these reasons, Model Maker uses a `Dataset` class to organize training data and feed it to the retraining process.\n",
        "\n",
        "To create a dataset, use the `Dataset.from_folder` method to load the data located at `image_path` and split it into training, testing, and validation groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uELMchkggUMP"
      },
      "outputs": [],
      "source": [
        "data = image_classifier.Dataset.from_folder(image_path)\n",
        "train_data, remaining_data = data.split(0.8)\n",
        "test_data, validation_data = remaining_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6nWxtN-QZGj"
      },
      "source": [
        "In this example, 80% of the data is used for training, with the remaining data split in half, so that 10% of the total is used for testing, and 10% for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhUcNVuLUzB1"
      },
      "source": [
        "## Retrain model\n",
        "\n",
        "Once you have completed preparing your data, you can begin retraining a model to build a new classification layer that can recognize the items types, or classes, defined by your training data. This type of model modification is called [transfer learning](https://www.wikipedia.org/wiki/Transfer_learning). The instructions below use the data prepared in the previous section to retrain an image classification model to recognize different types of flowers.\n",
        "\n",
        "**Note:** For this type of model, the retraining process causes the model to forget any classes it was previously able to recognize. Once the retraining is complete, the new model can *only* recognize classes trained from the new dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dv3H2bKO9L9"
      },
      "source": [
        "### Set retraining options\n",
        "\n",
        "There are a few required settings to run a retraining aside from your training dataset: output directory for the model and the model architecture. Use `HParams` object `export_dir` parameter to specify a model output directory. Use the `SupportedModels` class to specify the model architecture. The image classifier solution supports the following model architectures:\n",
        "\n",
        "- `MobileNet-V2`\n",
        "- `EfficientNet-Lite0`\n",
        "- `EfficientNet-Lite2`\n",
        "- `EfficientNet-Lite4`\n",
        "\n",
        "To set the required parameters, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B0sz4PTMUVZ"
      },
      "outputs": [],
      "source": [
        "# spec = image_classifier.SupportedModels.MOBILENET_V2\n",
        "spec = image_classifier.SupportedModels.EFFICIENTNET_LITE4\n",
        "hparams = image_classifier.HParams(export_dir=\"exported_model\")\n",
        "options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h47-hERKUOks"
      },
      "source": [
        "This example code uses MobileNetV2 model architecture, which you can learn more about from the  [MobileNetV2](https://arxiv.org/abs/1801.04381) research paper. The retraining process has many additional options, however most of them are set for you automatically. You can learn about these optional parameters in the [Retraining parameters](#retraining_parameters) section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLsv-abU4cr"
      },
      "source": [
        "### Run retraining\n",
        "\n",
        "With your training dataset and retraining options prepared, you are ready to start the retraining process. This process is resource intensive and can take a few minutes to a few hours depending on your available compute resources. Using a Google Colab environment with standard CPU processing, the example retraining below takes about 20 minutes to train on approximately 4000 images. You can typically decrease your training time by using GPU processors.\n",
        "\n",
        "To begin the retraining process, use the `create()` method with dataset and options you previously defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkbUY97gNP2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3ca4a2-2156-4b3b-8c7c-9fa8ff4a51eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1280)              11837936  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 2562      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11840498 (45.17 MB)\n",
            "Trainable params: 2562 (10.01 KB)\n",
            "Non-trainable params: 11837936 (45.16 MB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_bicubic (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.BICUBIC...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1120/1120 [==============================] - 323s 285ms/step - loss: 0.6781 - accuracy: 0.5826 - val_loss: 0.6440 - val_accuracy: 0.6679\n",
            "Epoch 2/10\n",
            "1120/1120 [==============================] - 304s 271ms/step - loss: 0.6456 - accuracy: 0.6438 - val_loss: 0.5940 - val_accuracy: 0.7607\n",
            "Epoch 3/10\n",
            "1120/1120 [==============================] - 318s 284ms/step - loss: 0.6018 - accuracy: 0.7080 - val_loss: 0.5670 - val_accuracy: 0.7714\n",
            "Epoch 4/10\n",
            "1120/1120 [==============================] - 314s 281ms/step - loss: 0.5886 - accuracy: 0.7295 - val_loss: 0.5404 - val_accuracy: 0.7821\n",
            "Epoch 5/10\n",
            "1120/1120 [==============================] - 316s 282ms/step - loss: 0.5715 - accuracy: 0.7348 - val_loss: 0.5264 - val_accuracy: 0.7893\n",
            "Epoch 6/10\n",
            "1120/1120 [==============================] - 313s 279ms/step - loss: 0.5642 - accuracy: 0.7478 - val_loss: 0.5155 - val_accuracy: 0.7929\n",
            "Epoch 7/10\n",
            "1120/1120 [==============================] - 318s 284ms/step - loss: 0.5514 - accuracy: 0.7580 - val_loss: 0.5083 - val_accuracy: 0.8214\n",
            "Epoch 8/10\n",
            "1120/1120 [==============================] - 305s 272ms/step - loss: 0.5503 - accuracy: 0.7567 - val_loss: 0.5047 - val_accuracy: 0.8107\n",
            "Epoch 9/10\n",
            "1120/1120 [==============================] - 312s 278ms/step - loss: 0.5484 - accuracy: 0.7603 - val_loss: 0.4954 - val_accuracy: 0.8214\n",
            "Epoch 10/10\n",
            "1120/1120 [==============================] - 304s 272ms/step - loss: 0.5395 - accuracy: 0.7723 - val_loss: 0.4925 - val_accuracy: 0.8214\n"
          ]
        }
      ],
      "source": [
        "model = image_classifier.ImageClassifier.create(\n",
        "    train_data = train_data,\n",
        "    validation_data = validation_data,\n",
        "    options=options,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0-ZzkN_VH4K"
      },
      "source": [
        "### Evaluate performance\n",
        "\n",
        "After retraining the model, you should evaluate it on a test dataset, which is typically a portion of your original dataset not used during training. Accuracy levels between 0.8 and 0.9 are generally considered very good, but your use case requirements may differ. You should also consider how fast the model can produce an inference. Higher accuracy frequently comes at the cost of longer inference times.\n",
        "\n",
        "To run an evaluation of the example model, run it against the test portion of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wjMzqWZQ9oV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c11d5b-2468-4a20-e960-630950456949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 55s 6s/step - loss: 0.4759 - accuracy: 0.8393\n",
            "Test loss:0.47591879963874817, Test accuracy:0.8392857313156128\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(test_data)\n",
        "print(f'Test loss:{loss}, Test accuracy:{acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO-WVLaIdlZJ"
      },
      "source": [
        "**Caution:** While high accuracy of a model is a common goal for machine learning models, you should be cautious of training to a point of [overfitting](https://en.wikipedia.org/wiki/Overfitting), which causes the model to perform extremely well with its training data, but quite poorly on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl92JHASVLSX"
      },
      "source": [
        "## Export model\n",
        "\n",
        "After retraining a model, you must export it to Tensorflow Lite model format to use it with the MediaPipe in your application. The export process generates required model metadata, as well as a classification label file.\n",
        "\n",
        "To export the retrained model for use in your application, use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sGv08Knh3_D"
      },
      "outputs": [],
      "source": [
        "model.export_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNfK9UnAgwmp"
      },
      "source": [
        "Use the follow commands with Google Colab to list model and download it to your development environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hKOJXwH57bL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e858b3e9-1057-483a-cca7-0e7d9db87ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint  metadata.json  model.tflite  summaries\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ac007abd-7565-47c1-b9fa-2bfac5d95f04\", \"model.tflite\", 46756301)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!ls exported_model\n",
        "files.download('exported_model/model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdUDt67aAKKD"
      },
      "source": [
        "## Model tuning\n",
        "\n",
        "You can use the MediaPipe Model Maker tool to further improve and adjust the  model retraining with configuation options and performance techniques such as data quantization. *These steps are optional.* Model Maker uses reasonable default settings for all of the training configuration parameters, but if you want to further tune the model retraining, the instructions below describe the available options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tTuqWThjr7V"
      },
      "source": [
        "### Retraining parameters\n",
        "\n",
        "You can further customize how the retraining process runs to adjust training time and potentially increase the retrained model's performance. *These parameters are optional*. Use the `ImageClassifierModelOptions` class and the `HParams` class to set these additional options.\n",
        "\n",
        "Use the `ImageClassifierModelOptions` class parameters to customize the existing model. It has the following customizable parameter that affects model accuracy:\n",
        "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
        "\n",
        "Use the `HParams` class to customize other parameters related to training and saving the model:\n",
        "\n",
        "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
        "* `batch_size`: Batch size for training. Defaults to 2.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
        "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
        "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
        "* `do_fine_tuning`: If true, the base module is trained together with the classification layer on top. This defaults to False, which means only the classification layer is trained and pre-trained weights for the base module are frozen.\n",
        "* `l1_regularizer`: A regularizer that applies a L1 regularization penalty. Defaults to 0.0.\n",
        "* `l2_regularizer`: A regularizer that applies a L2 regularization penalty. Defaults to 0.0001.\n",
        "* `label_smoothing`: Amount of label smoothing to apply. See [`tf.keras.losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses) for more details. Defaults to 0.1.\n",
        "* `do_data_augmentation`: Whether or not the training dataset is augmented by applying random transformations such as cropping, flipping, etc. See [utils.image_preprocessing](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image) for details. Defaults to True.\n",
        "* `decay_samples`: Number of training samples used to calculate the decay steps and create the training optimizer. Defaults to 2,560,000.\n",
        "* `warmup_epochs`: Number of warmup steps for a linear increasing warmup schedule on the learning rate. Used to set up warmup schedule by `model_util.WarmUp`. Defaults to 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOPf1ysXE4AP"
      },
      "source": [
        "The following example code trains a model with more epochs and a higher dropout rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Ar9Os1E3de"
      },
      "outputs": [],
      "source": [
        "hparams=image_classifier.HParams(epochs=15, export_dir=\"exported_model_2\")\n",
        "options = image_classifier.ImageClassifierOptions(supported_model=spec, hparams=hparams)\n",
        "options.model_options = image_classifier.ModelOptions(dropout_rate = 0.07)\n",
        "model_2 = image_classifier.ImageClassifier.create(\n",
        "    train_data = train_data,\n",
        "    validation_data = validation_data,\n",
        "    options=options,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OrzB8EtFQIf"
      },
      "source": [
        "To evaluate the newly trained model above, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giS6EqkfAMkZ"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXU-IkRSrHgL"
      },
      "source": [
        "For more information on the general performance of the supported models, refer to the [Performance benchmarks](#performance_benchmarks) section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4QQpsfBCFuQ"
      },
      "source": [
        "### Model quantization\n",
        "\n",
        "Post-training model quantization is a model modification technique that can reduce the model size and improve the speed of predictions with only a relatively minor decrease in accuracy. This approach reduces the size of the data processed by the model, for example by transforming 32-bit floating point numbers to 8-bit integers. This technique is widely used to further optimize models after the training process.\n",
        "\n",
        "This section of the guide explains how to apply quantization to your retrained model. This optimization must be done as part of the Model Maker model export process, and cannot be performed on an exported model. The following example demonstrates how to use this approach to apply `int8` quantization to a retrained model. For more information on post-training quantization, see the [TensorFlow Lite](https://www.tensorflow.org/lite/performance/post_training_quantization) documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaW-YGAGyBnx"
      },
      "source": [
        "Import the MediaPipe Model Maker quantization module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8aqeE68HUX6"
      },
      "outputs": [],
      "source": [
        "from mediapipe_model_maker import quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-yy3cD_Hzxn"
      },
      "source": [
        "Define a `QuantizationConfig` object using the `for_int8()` class method. This configuration modifies a trained model to use 8-bit integers instead of larger data types, such as 32-bit floating point numbers. You can further customize the quantization process by setting additional parameters for the `QuantizationConfig` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOmTokRQBzGs"
      },
      "outputs": [],
      "source": [
        "quantization_config = quantization.QuantizationConfig.for_int8(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zscQ5nxIKEam"
      },
      "source": [
        "The `for_int8()` method requires a representative dataset, while the `dynamic()` and `for_float16()` quantization methods do not. The quantization process uses the representative dataset to perform model modification and you typically use your existing training dataset for this purpose. For more information on the process and options, see the TensorFlow Lite [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) guide.\n",
        "\n",
        "Export the model using the additional `quantization_config` object to apply post-training quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss8OeqJhKEoZ"
      },
      "outputs": [],
      "source": [
        "model.export_model(model_name=\"model_int8.tflite\", quantization_config=quantization_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlJdE9KLNZlj"
      },
      "source": [
        "After running this command, you should have a new `model_int8.tflite` model file. This new, quantized model should be significantly smaller than the `model.tflite` file. You can compare the sizes using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc7Ja6YzNZVJ"
      },
      "outputs": [],
      "source": [
        "!ls -lh exported_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfr-8r5gtqNh"
      },
      "source": [
        "## Performance benchmarks\n",
        "\n",
        "Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same flowers dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n",
        "- The test accuracy column reflects models which were trained with the default parameters. To optimize model performance, experiment with different model and retraining parameters in order to obtain the highest test accuracy. Refer to the [Retraining parameters](#retraining_parameters) section for more information on customizing these settings.\n",
        "- The larger model architectures, such as EfficientNet_Lite4, may not acheive the highest test accuracy on simpler datasets like the flowers dataset used this notebook. Research suggests that these larger model architecture can outperform the others on more complex datasets like ImageNet, for more information, see [EfficientNet paper](https://arxiv.org/pdf/1905.11946.pdf). The ImageNet dataset is more complex, with over a million training images and 1000 classes, while the flowers dataset has only 3670 training images and 5 classses.\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Model architecture</th>\n",
        "<th>Test Accuracy</th>\n",
        "<th>Model Size</th>\n",
        "<th>CPU 1 Thread Latency(Pixel 6)</th>\n",
        "<th>GPU Latency(Pixel 6)</th>\n",
        "<th>EdgeTPULatency(Pixel 6)</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>MobileNet_V2</td>\n",
        "<td>85.4%</td>\n",
        "<td><strong>8.9MB</strong></td>\n",
        "<td>29.12</td>\n",
        "<td>77.77</td>\n",
        "<td>31.14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite0</td>\n",
        "<td>91.3%</td>\n",
        "<td>13.5MB</td>\n",
        "<td><strong>15.6</strong></td>\n",
        "<td><strong>9.25</strong></td>\n",
        "<td><strong>16.72</strong></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite2</td>\n",
        "<td><strong>91.5%</strong></td>\n",
        "<td>19.2MB</td>\n",
        "<td>35.2</td>\n",
        "<td>13.94</td>\n",
        "<td>37.52</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>EfficientNet_Lite4</td>\n",
        "<td>90.8%</td>\n",
        "<td>46.8MB</td>\n",
        "<td>103.16</td>\n",
        "<td>23.14</td>\n",
        "<td>114.67</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}